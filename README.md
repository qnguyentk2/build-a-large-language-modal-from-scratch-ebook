# build-a-large-language-modal-from-scratch-ebook

<img width="1625" height="966" alt="image" src="https://github.com/user-attachments/assets/0fad6a8e-1d15-4106-a28a-fa03f687f075" />

## Chapter 1 - Understanding large language models

LLM khÃ´ng suy nghÄ© â†’ nÃ³ mÃ´ phá»ng káº¿t quáº£ cá»§a tÆ° duy thÃ´ng qua thá»‘ng kÃª ngÃ´n ngá»¯.
LLM giÃºp mÃ¡y tÃ­nh táº¡o ra cÃ¢u tráº£ lá»i báº±ng cÃ¡ch dá»± Ä‘oÃ¡n vÃ  sinh cÃ¡c tá»« (token) sao cho phÃ¹ há»£p vá»›i ngá»¯ cáº£nh ngÃ´n ngá»¯ mÃ  con ngÆ°á»i thÆ°á»ng dÃ¹ng; vá» báº£n cháº¥t, nÃ³ khÃ´ng suy nghÄ© hay cÃ³ Ã½ thá»©c nhÆ° con ngÆ°á»i khi nÃ³i ra cÃ¢u Ä‘Ã³.

Nhá» nhá»¯ng tiáº¿n bá»™ trong há»c sÃ¢u (deep learning), LLM Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn lÆ°á»£ng dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“. Viá»‡c huáº¥n luyá»‡n quy mÃ´ lá»›n giÃºp mÃ´ hÃ¬nh náº¯m báº¯t ngá»¯ cáº£nh vÃ  sáº¯c thÃ¡i ngÃ´n ngá»¯ tá»‘t hÆ¡n, tá»« Ä‘Ã³ cáº£i thiá»‡n máº¡nh hiá»‡u suáº¥t trÃªn nhiá»u bÃ i toÃ¡n NLP nhÆ° dá»‹ch thuáº­t, phÃ¢n tÃ­ch cáº£m xÃºc vÃ  tráº£ lá»i cÃ¢u há»i.

**KhÃ´ng pháº£i kiáº¿n trÃºc â€œthÃ´ng minh hÆ¡nâ€ â†’ mÃ  lÃ  kiáº¿n trÃºc + dá»¯ liá»‡u + scale táº¡o ra kháº£ nÄƒng má»›i.**

```
More data + bigger models + longer training
â†’ better language representations
â†’ emergent behaviors (reasoning, summarization, QA)
```

> Emergent behaviors lÃ  nhá»¯ng kháº£ nÄƒng â€œmá»c raâ€ ngoÃ i dá»± kiáº¿n khi mÃ´ hÃ¬nh Ä‘Æ°á»£c scale Ä‘á»§ lá»›n (nhiá»u tham sá»‘, nhiá»u dá»¯ liá»‡u, huáº¥n luyá»‡n lÃ¢u), chá»© khÃ´ng pháº£i do con ngÆ°á»i chá»§ Ä‘á»™ng code vÃ o.
>> VÃ­ dá»¥ Ä‘á»i thÆ°á»ng (ráº¥t dá»… hiá»ƒu)
>> 
>> ğŸœ Má»™t con kiáº¿n â†’ khÃ´ng thÃ´ng minh
>> 
>> ğŸœğŸœğŸœ Cáº£ Ä‘Ã n kiáº¿n â†’ biáº¿t tÃ¬m Ä‘Æ°á»ng, xÃ¢y tá»•, phÃ¢n cÃ´ng
>> 
>> ğŸ‘‰ KhÃ´ng con kiáº¿n nÃ o â€œbiáº¿tâ€ chiáº¿n lÆ°á»£c,
>> 
>> ğŸ‘‰ nhÆ°ng hÃ nh vi táº­p thá»ƒ tá»± xuáº¥t hiá»‡n â†’ emergent behavior.


### Váº­y LLM lÃ  gÃ¬? 
**LLM (mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n)** lÃ  má»™t máº¡ng nÆ¡-ron Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hiá»ƒu, táº¡o sinh vÃ  pháº£n há»“i vÄƒn báº£n giá»‘ng con ngÆ°á»i. CÃ¡c mÃ´ hÃ¬nh nÃ y lÃ  nhá»¯ng máº¡ng nÆ¡-ron sÃ¢u, Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn khá»‘i lÆ°á»£ng dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“ â€” Ä‘Ã´i khi bao phá»§ má»™t pháº§n ráº¥t lá»›n cá»§a toÃ n bá»™ vÄƒn báº£n cÃ´ng khai hiá»‡n cÃ³ trÃªn internet.

Chá»¯ â€˜lá»›nâ€™ trong â€˜mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›nâ€™ vá»«a Ä‘á» cáº­p Ä‘áº¿n kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh xÃ©t theo sá»‘ lÆ°á»£ng tham sá»‘, vá»«a Ä‘á» cáº­p Ä‘áº¿n bá»™ dá»¯ liá»‡u cá»±c ká»³ lá»›n mÃ  nÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn Ä‘Ã³. Nhá»¯ng mÃ´ hÃ¬nh nhÆ° váº­y thÆ°á»ng cÃ³ hÃ ng chá»¥c, tháº­m chÃ­ hÃ ng trÄƒm tá»· tham sá»‘ â€” tá»©c cÃ¡c trá»ng sá»‘ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh trong máº¡ng â€” vÃ  chÃºng Ä‘Æ°á»£c tá»‘i Æ°u trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t chuá»—i.

Viá»‡c dá»± Ä‘oÃ¡n tá»« tiáº¿p theo lÃ  há»£p lÃ½ vÃ¬ nÃ³ táº­n dá»¥ng báº£n cháº¥t tuáº§n tá»± vá»‘n cÃ³ cá»§a ngÃ´n ngá»¯ Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c cÃ¡ch náº¯m báº¯t ngá»¯ cáº£nh, cáº¥u trÃºc vÃ  cÃ¡c má»‘i quan há»‡ bÃªn trong vÄƒn báº£n. ÄÃ¢y lÃ  má»™t nhiá»‡m vá»¥ ráº¥t Ä‘Æ¡n giáº£n, vÃ¬ váº­y nhiá»u nhÃ  nghiÃªn cá»©u cáº£m tháº¥y báº¥t ngá» khi nÃ³ cÃ³ thá»ƒ táº¡o ra nhá»¯ng mÃ´ hÃ¬nh máº¡nh Ä‘áº¿n váº­y. Trong cÃ¡c chÆ°Æ¡ng sau, chÃºng ta sáº½ tháº£o luáº­n vÃ  triá»ƒn khai quy trÃ¬nh huáº¥n luyá»‡n dá»± Ä‘oÃ¡n tá»« tiáº¿p theo theo tá»«ng bÆ°á»›c má»™t.

LLM sá»­ dá»¥ng má»™t kiáº¿n trÃºc gá»i lÃ  Transformer, cho phÃ©p mÃ´ hÃ¬nh táº­p trung sá»± chÃº Ã½ má»™t cÃ¡ch chá»n lá»c vÃ o cÃ¡c pháº§n khÃ¡c nhau cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o khi Ä‘Æ°a ra dá»± Ä‘oÃ¡n. Nhá» Ä‘Ã³, chÃºng Ä‘áº·c biá»‡t hiá»‡u quáº£ trong viá»‡c xá»­ lÃ½ nhá»¯ng sáº¯c thÃ¡i vÃ  Ä‘á»™ phá»©c táº¡p cá»§a ngÃ´n ngá»¯ con ngÆ°á»i.
Kiáº¿n trÃºc Transformer giÃºp LLM â€˜chÃº Ã½ Ä‘Ãºng chá»—â€™ trong vÄƒn báº£n Ä‘áº§u vÃ o, nÃªn mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu vÃ  xá»­ lÃ½ tá»‘t hÆ¡n cÃ¡c má»‘i quan há»‡ vÃ  sáº¯c thÃ¡i tinh táº¿ cá»§a ngÃ´n ngá»¯.

VÃ¬ cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) cÃ³ kháº£ nÄƒng táº¡o sinh vÄƒn báº£n, nÃªn chÃºng thÆ°á»ng Ä‘Æ°á»£c xem lÃ  má»™t dáº¡ng trÃ­ tuá»‡ nhÃ¢n táº¡o táº¡o sinh (generative artificial intelligence), hay thÆ°á»ng Ä‘Æ°á»£c viáº¿t táº¯t lÃ  generative AI hoáº·c GenAI

<img width="1349" height="481" alt="image" src="https://github.com/user-attachments/assets/25ef83d5-ff6e-4942-ab1c-5c7ea528b56e" />

>1.1, AI encompasses the broader field of creating machines that can perform tasks requiring human-like intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning.

### sÆ¡ Ä‘á»“ phÃ¢n cáº¥p:

```
Artificial Intelligence (AI)
â”‚
â”œâ”€â”€ Machine Learning (ML)
â”‚   â”‚
â”‚   â”œâ”€â”€ Deep Learning (DL)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Neural Networks
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ Large Language Models (LLMs)
â”‚   â”‚   â”‚       (Transformer-based models)

```

***1ï¸âƒ£ Artificial Intelligence (AI)***
Má»¥c tiÃªu lá»›n: táº¡o mÃ¡y mÃ³c cÃ³ hÃ nh vi giá»‘ng trÃ­ tuá»‡ con ngÆ°á»i   
Bao gá»“m:

     - hiá»ƒu ngÃ´n ngá»¯
     
     - nháº­n diá»‡n máº«u
     
     - ra quyáº¿t Ä‘á»‹nh

     ğŸ‘‰ AI = khÃ¡i niá»‡m bao trÃ¹m

***2ï¸âƒ£ Machine Learning (ML)***

     - Má»™t nhÃ¡nh cá»§a AI
   
     - MÃ¡y khÃ´ng cáº§n code rule cá»©ng
   
     - Há»c tá»« dá»¯ liá»‡u â†’ tÃ¬m quy luáº­t
   
     ğŸ‘‰ â€œLearn from data, not from rulesâ€


***3ï¸âƒ£ Deep Learning (DL)***

   Má»™t nhÃ¡nh cá»§a ML
   
     - Dá»±a trÃªn máº¡ng nÆ¡-ron nhiá»u táº§ng
   
     - Scale tá»‘t khi:
   
        - dá»¯ liá»‡u lá»›n
   
        - compute máº¡nh
   
     ğŸ‘‰ DL = ML + neural networks + scale


***4ï¸âƒ£ Large Language Models (LLMs)***

   Má»™t á»©ng dá»¥ng cá»¥ thá»ƒ cá»§a Deep Learning
   
        Táº­p trung vÃ o ngÃ´n ngá»¯
   
        ThÆ°á»ng dÃ¹ng:
   
             kiáº¿n trÃºc Transformer
   
             bÃ i toÃ¡n next-word prediction
   
        ğŸ‘‰ LLM = Deep Neural Network + Transformer + Massive Text Data



### Keyword:
1. [Deep neural network models (DNN models):](https://chatgpt.com/g/g-p-696e03d1cfd481918a4ca9cdc44a493c-build-a-large-language-model-from-scratch/c/696e03d8-ba1c-8332-a092-3f3c2e82bdb3) 
Deep Neural Network lÃ  má»™t há»‡ thá»‘ng gá»“m nhiá»u táº§ng toÃ¡n há»c ná»‘i tiáº¿p nhau, há»c cÃ¡ch Ã¡nh xáº¡ input â†’ output báº±ng cÃ¡ch tá»± Ä‘iá»u chá»‰nh trá»ng sá»‘ thÃ´ng qua dá»¯ liá»‡u, thay vÃ¬ viáº¿t rule thá»§ cÃ´ng.

ğŸ‘‰ Trá»ng sá»‘ (weights) khÃ´ng â€œtá»± nhiÃªn mÃ  cÃ³â€ â€” nÃ³ Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn, rá»“i Ä‘Æ°á»£c há»c dáº§n tá»« dá»¯ liá»‡u.

2. [Selective attention](https://chatgpt.com/g/g-p-696e03d1cfd481918a4ca9cdc44a493c-build-a-large-language-model-from-scratch/c/696e0f12-05f8-832b-a099-1a7f7ac94294)
   Selective attention trong kiáº¿n trÃºc Transformer lÃ  cÆ¡ cháº¿ cho phÃ©p mÃ´ hÃ¬nh chá»n lá»c nhá»¯ng pháº§n thÃ´ng tin quan trá»ng nháº¥t Ä‘á»ƒ táº­p trung, thay vÃ¬ xá»­ lÃ½ má»i thá»© má»™t cÃ¡ch Ä‘á»“ng Ä‘á»u.


   **Khi Ä‘á»c má»™t chuá»—i (cÃ¢u, token, vector):**

    Transformer khÃ´ng â€œnhÃ¬n Ä‘á»uâ€ táº¥t cáº£ token.

    á» má»—i token, mÃ´ hÃ¬nh quyáº¿t Ä‘á»‹nh token nÃ o Ä‘Ã¡ng chÃº Ã½ hÆ¡n (liÃªn quan hÆ¡n) Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin.

    Viá»‡c â€œchá»n lá»câ€ nÃ y diá»…n ra tá»± Ä‘á»™ng, thÃ´ng qua trá»ng sá»‘ attention Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh train.


> **VÃ­ dá»¥**
> â€œCon mÃ¨o náº±m trÃªn táº¥m tháº£m vÃ¬ nÃ³ ráº¥t áº¥m.â€
>
> 
> Khi xá»­ lÃ½ tá»« â€œnÃ³â€:
>
> 
> Attention sáº½ táº­p trung máº¡nh vÃ o â€œtáº¥m tháº£mâ€,
> 
> Ãt chÃº Ã½ hÆ¡n tá»›i â€œconâ€, â€œnáº±mâ€, â€œvÃ¬â€, â€¦
> 
> â†’ Transformer chá»n lá»c ngá»¯ cáº£nh cÃ³ Ã½ nghÄ©a.
