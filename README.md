# build-a-large-language-modal-from-scratch-ebook

<img width="1625" height="966" alt="image" src="https://github.com/user-attachments/assets/0fad6a8e-1d15-4106-a28a-fa03f687f075" />

## Chapter 1 - Understanding large language models

LLM khÃ´ng suy nghÄ© â†’ nÃ³ mÃ´ phá»ng káº¿t quáº£ cá»§a tÆ° duy thÃ´ng qua thá»‘ng kÃª ngÃ´n ngá»¯.
LLM giÃºp mÃ¡y tÃ­nh táº¡o ra cÃ¢u tráº£ lá»i báº±ng cÃ¡ch dá»± Ä‘oÃ¡n vÃ  sinh cÃ¡c tá»« (token) sao cho phÃ¹ há»£p vá»›i ngá»¯ cáº£nh ngÃ´n ngá»¯ mÃ  con ngÆ°á»i thÆ°á»ng dÃ¹ng; vá» báº£n cháº¥t, nÃ³ khÃ´ng suy nghÄ© hay cÃ³ Ã½ thá»©c nhÆ° con ngÆ°á»i khi nÃ³i ra cÃ¢u Ä‘Ã³.

Nhá» nhá»¯ng tiáº¿n bá»™ trong há»c sÃ¢u (deep learning), LLM Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn lÆ°á»£ng dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“. Viá»‡c huáº¥n luyá»‡n quy mÃ´ lá»›n giÃºp mÃ´ hÃ¬nh náº¯m báº¯t ngá»¯ cáº£nh vÃ  sáº¯c thÃ¡i ngÃ´n ngá»¯ tá»‘t hÆ¡n, tá»« Ä‘Ã³ cáº£i thiá»‡n máº¡nh hiá»‡u suáº¥t trÃªn nhiá»u bÃ i toÃ¡n NLP nhÆ° dá»‹ch thuáº­t, phÃ¢n tÃ­ch cáº£m xÃºc vÃ  tráº£ lá»i cÃ¢u há»i.

**KhÃ´ng pháº£i kiáº¿n trÃºc â€œthÃ´ng minh hÆ¡nâ€ â†’ mÃ  lÃ  kiáº¿n trÃºc + dá»¯ liá»‡u + scale táº¡o ra kháº£ nÄƒng má»›i.**

```
More data + bigger models + longer training
â†’ better language representations
â†’ emergent behaviors (reasoning, summarization, QA)
```

> Emergent behaviors lÃ  nhá»¯ng kháº£ nÄƒng â€œmá»c raâ€ ngoÃ i dá»± kiáº¿n khi mÃ´ hÃ¬nh Ä‘Æ°á»£c scale Ä‘á»§ lá»›n (nhiá»u tham sá»‘, nhiá»u dá»¯ liá»‡u, huáº¥n luyá»‡n lÃ¢u), chá»© khÃ´ng pháº£i do con ngÆ°á»i chá»§ Ä‘á»™ng code vÃ o.
>> VÃ­ dá»¥ Ä‘á»i thÆ°á»ng (ráº¥t dá»… hiá»ƒu)
>> 
>> ğŸœ Má»™t con kiáº¿n â†’ khÃ´ng thÃ´ng minh
>> 
>> ğŸœğŸœğŸœ Cáº£ Ä‘Ã n kiáº¿n â†’ biáº¿t tÃ¬m Ä‘Æ°á»ng, xÃ¢y tá»•, phÃ¢n cÃ´ng
>> 
>> ğŸ‘‰ KhÃ´ng con kiáº¿n nÃ o â€œbiáº¿tâ€ chiáº¿n lÆ°á»£c,
>> 
>> ğŸ‘‰ nhÆ°ng hÃ nh vi táº­p thá»ƒ tá»± xuáº¥t hiá»‡n â†’ emergent behavior.

LLM sá»­ dá»¥ng má»™t kiáº¿n trÃºc gá»i lÃ  Transformer, cho phÃ©p mÃ´ hÃ¬nh táº­p trung sá»± chÃº Ã½ má»™t cÃ¡ch chá»n lá»c vÃ o cÃ¡c pháº§n khÃ¡c nhau cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o khi Ä‘Æ°a ra dá»± Ä‘oÃ¡n. Nhá» Ä‘Ã³, chÃºng Ä‘áº·c biá»‡t hiá»‡u quáº£ trong viá»‡c xá»­ lÃ½ nhá»¯ng sáº¯c thÃ¡i vÃ  Ä‘á»™ phá»©c táº¡p cá»§a ngÃ´n ngá»¯ con ngÆ°á»i.
Kiáº¿n trÃºc Transformer giÃºp LLM â€˜chÃº Ã½ Ä‘Ãºng chá»—â€™ trong vÄƒn báº£n Ä‘áº§u vÃ o, nÃªn mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu vÃ  xá»­ lÃ½ tá»‘t hÆ¡n cÃ¡c má»‘i quan há»‡ vÃ  sáº¯c thÃ¡i tinh táº¿ cá»§a ngÃ´n ngá»¯.


### Keyword:
1. [Deep neural network models (DNN models):](https://chatgpt.com/g/g-p-696e03d1cfd481918a4ca9cdc44a493c-build-a-large-language-model-from-scratch/c/696e03d8-ba1c-8332-a092-3f3c2e82bdb3) 
Deep Neural Network lÃ  má»™t há»‡ thá»‘ng gá»“m nhiá»u táº§ng toÃ¡n há»c ná»‘i tiáº¿p nhau, há»c cÃ¡ch Ã¡nh xáº¡ input â†’ output báº±ng cÃ¡ch tá»± Ä‘iá»u chá»‰nh trá»ng sá»‘ thÃ´ng qua dá»¯ liá»‡u, thay vÃ¬ viáº¿t rule thá»§ cÃ´ng.

ğŸ‘‰ Trá»ng sá»‘ (weights) khÃ´ng â€œtá»± nhiÃªn mÃ  cÃ³â€ â€” nÃ³ Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn, rá»“i Ä‘Æ°á»£c há»c dáº§n tá»« dá»¯ liá»‡u.

2. [Selective attention] (https://chatgpt.com/g/g-p-696e03d1cfd481918a4ca9cdc44a493c-build-a-large-language-model-from-scratch/c/696e0f12-05f8-832b-a099-1a7f7ac94294)
   Selective attention trong kiáº¿n trÃºc Transformer lÃ  cÆ¡ cháº¿ cho phÃ©p mÃ´ hÃ¬nh chá»n lá»c nhá»¯ng pháº§n thÃ´ng tin quan trá»ng nháº¥t Ä‘á»ƒ táº­p trung, thay vÃ¬ xá»­ lÃ½ má»i thá»© má»™t cÃ¡ch Ä‘á»“ng Ä‘á»u.


   **Khi Ä‘á»c má»™t chuá»—i (cÃ¢u, token, vector):**

    Transformer khÃ´ng â€œnhÃ¬n Ä‘á»uâ€ táº¥t cáº£ token.

    á» má»—i token, mÃ´ hÃ¬nh quyáº¿t Ä‘á»‹nh token nÃ o Ä‘Ã¡ng chÃº Ã½ hÆ¡n (liÃªn quan hÆ¡n) Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin.

    Viá»‡c â€œchá»n lá»câ€ nÃ y diá»…n ra tá»± Ä‘á»™ng, thÃ´ng qua trá»ng sá»‘ attention Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh train.


   **VÃ­ dá»¥**
   â€œCon mÃ¨o náº±m trÃªn táº¥m tháº£m vÃ¬ nÃ³ ráº¥t áº¥m.â€

   Khi xá»­ lÃ½ tá»« â€œnÃ³â€:

    Attention sáº½ táº­p trung máº¡nh vÃ o â€œtáº¥m tháº£mâ€,

    Ãt chÃº Ã½ hÆ¡n tá»›i â€œconâ€, â€œnáº±mâ€, â€œvÃ¬â€, â€¦

    â†’ Transformer chá»n lá»c ngá»¯ cáº£nh cÃ³ Ã½ nghÄ©a.
